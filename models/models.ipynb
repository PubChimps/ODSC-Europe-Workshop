{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ODSC Workshop Part 2 - Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in the tokenized dataset from the previous notebook, it may have to be moved to your current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('X_train.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "X_test = np.load('X_test.npy')\n",
    "y_test = np.load('y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input_dim is equal to the length of the tokenizer.word_index plus 1. This creates over 45,000 nodes in the first layer of our neural network, which will take too long to train. Luckily, as is the case with many text datasets, when can omit a large portion of the index as the least frequently used words are very sparse, and still produce an accruate model that trains quickly\n",
    "\n",
    "##### Begin executing the cell below, but note the eta for the training time for each epoch. This is too long. Stop the execution of the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 45785\n",
    "embedding_dim = 1000\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=X_train.shape[1]))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(10, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "                    epochs=3,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=10)\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rebuilind a new tokenizer, this time with half of the words of the previous one. This will be produce an index of the top 50% of words that occur most frequently\n",
    "\n",
    "#### Rerun the same model and note the difference and training time and the effects on model accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = np.load('code.npy')\n",
    "labels = np.load('labels.npy')\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=45785*.5)\n",
    "tokenizer.fit_on_texts(code)\n",
    "vocab_size = len(tokenizer.word_index) + 1 \n",
    "maxlen = 1000\n",
    "\n",
    "code_train = code[:9180]\n",
    "code_test = code[9180:]\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(code_train)\n",
    "X_test = tokenizer.texts_to_sequences(code_test)\n",
    "\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "y_train = np.array(labels[:9180]).reshape(9180,1)\n",
    "y_test = np.array(labels[9180:]).reshape(2295,1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=maxlen))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(10, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "                    epochs=3,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=100)\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using tf.keras' embedding layer allows for the creation of 1D Text CNN's, which can be build just like other famous CNN's. Below is an example of a popular CNN for image classification, LeNet, built to classify text. The network will be trained and saved along with the tokenizer for model serving\n",
    "\n",
    "##### Execute the cell below and then use the remaining cells as a sandbox to test out different neural network archectectures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet = tf.keras.Sequential()\n",
    "lenet.add(tf.keras.layers.Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=maxlen))\n",
    "\n",
    "lenet.add(tf.keras.layers.Conv1D(filters=6, kernel_size=(3), activation='relu'))\n",
    "lenet.add(tf.keras.layers.AveragePooling1D())\n",
    "\n",
    "lenet.add(tf.keras.layers.Conv1D(filters=16, kernel_size=(3), activation='relu'))\n",
    "lenet.add(tf.keras.layers.AveragePooling1D())\n",
    "\n",
    "lenet.add(tf.keras.layers.Flatten())\n",
    "\n",
    "lenet.add(tf.keras.layers.Dense(units=120, activation='relu'))\n",
    "\n",
    "lenet.add(tf.keras.layers.Dense(units=84, activation='relu'))\n",
    "\n",
    "lenet.add(tf.keras.layers.Dense(units=1, activation = 'sigmoid'))\n",
    "lenet.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "lenet.fit(X_train, y_train,\n",
    "                    epochs=3,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=100)\n",
    "loss, accuracy = lenet.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = lenet.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "\n",
    "tf.saved_model.save(lenet, \"/tmp/lenet/1/\")\n",
    "\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SANDBOX - try different model architectures below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
